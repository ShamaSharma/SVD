{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNfbI+E74nRC5P1BCni9TvC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShamaSharma/SVD/blob/main/DevignML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Devign Vulnerability Detection - ML Baseline with Full Metrics and Visualizations\n",
        "Complete working version with all imports and error handling\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import sys\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datasets import load_dataset\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    accuracy_score,\n",
        "    confusion_matrix,\n",
        "    precision_recall_curve,\n",
        "    average_precision_score\n",
        ")\n",
        "import xgboost as xgb\n",
        "\n",
        "# Set plotting style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "# Configuration\n",
        "TRAIN_SIZE = None  # None = use full dataset, or set to integer (e.g., 5000) for subset\n",
        "MAX_FEATURES = 5000\n",
        "TEST_SIZE = 0.2\n",
        "RANDOM_STATE = 42\n",
        "RESULTS_DIR = \"ml_results_full_metrics\"\n",
        "PLOTS_DIR = \"ml_plots\"\n",
        "\n",
        "# -----------------------------\n",
        "# Step 1: Load Devign dataset\n",
        "# -----------------------------\n",
        "def load_data(train_size=TRAIN_SIZE):\n",
        "    \"\"\"Load and validate dataset\"\"\"\n",
        "    try:\n",
        "        print(\"Loading Devign dataset...\")\n",
        "        ds = load_dataset(\"DetectVul/devign\")\n",
        "\n",
        "        # Determine how many samples to use\n",
        "        available_samples = len(ds['train'])\n",
        "\n",
        "        if train_size is None:\n",
        "            train_size = available_samples\n",
        "            print(f\"Using FULL dataset: {train_size} samples\")\n",
        "        else:\n",
        "            if available_samples < train_size:\n",
        "                print(f\"Warning: Only {available_samples} samples available, using all.\")\n",
        "                train_size = available_samples\n",
        "            else:\n",
        "                print(f\"Using subset: {train_size} of {available_samples} samples\")\n",
        "\n",
        "        X_text = ds['train']['func'][:train_size]\n",
        "\n",
        "        # Handle label field - it might be 'label' or 'target'\n",
        "        if 'target' in ds['train'].column_names:\n",
        "            y = ds['train']['target'][:train_size]\n",
        "        else:\n",
        "            y = ds['train']['label'][:train_size]\n",
        "\n",
        "        # Convert labels if they're lists\n",
        "        if isinstance(y[0], list):\n",
        "            y = [label[0] if isinstance(label, list) else label for label in y]\n",
        "\n",
        "        # Convert to standard Python types\n",
        "        y = [int(label) for label in y]\n",
        "\n",
        "        # Validate we have both classes\n",
        "        unique_labels = set(y)\n",
        "        if len(unique_labels) < 2:\n",
        "            raise ValueError(f\"Dataset only contains one class: {unique_labels}\")\n",
        "\n",
        "        print(f\"Loaded {train_size} samples with {len(unique_labels)} classes\")\n",
        "        return X_text, y\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading dataset: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        sys.exit(1)\n",
        "\n",
        "# -----------------------------\n",
        "# Step 2: Convert code to features\n",
        "# -----------------------------\n",
        "def vectorize_data(X_text, max_features=MAX_FEATURES):\n",
        "    \"\"\"Convert text to feature vectors\"\"\"\n",
        "    try:\n",
        "        print(\"Vectorizing code...\")\n",
        "        vectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b', max_features=max_features)\n",
        "        X = vectorizer.fit_transform(X_text)\n",
        "        print(f\"Feature matrix shape: {X.shape}\")\n",
        "        return X, vectorizer\n",
        "    except Exception as e:\n",
        "        print(f\"Error during vectorization: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        sys.exit(1)\n",
        "\n",
        "# -----------------------------\n",
        "# Step 3: Define ML models\n",
        "# -----------------------------\n",
        "def get_models():\n",
        "    \"\"\"Return dictionary of ML models (SVM excluded for memory efficiency)\"\"\"\n",
        "    return {\n",
        "        \"Naive_Bayes\": MultinomialNB(),\n",
        "        \"Logistic_Regression\": LogisticRegression(max_iter=1000, random_state=RANDOM_STATE),\n",
        "        \"Random_Forest\": RandomForestClassifier(n_estimators=50, random_state=RANDOM_STATE, n_jobs=-1),\n",
        "        \"XGBoost\": xgb.XGBClassifier(eval_metric='logloss', random_state=RANDOM_STATE, use_label_encoder=False)\n",
        "    }\n",
        "\n",
        "# -----------------------------\n",
        "# Step 4: Calculate metrics\n",
        "# -----------------------------\n",
        "def calculate_metrics(y_test, y_pred, y_pred_proba=None):\n",
        "    \"\"\"Calculate all classification metrics\"\"\"\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    report_dict = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    # Handle binary and multiclass cases\n",
        "    if cm.shape == (2, 2):\n",
        "        tn, fp, fn, tp = cm.ravel()\n",
        "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
        "        fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
        "    else:\n",
        "        fpr = \"N/A (multiclass)\"\n",
        "        fnr = \"N/A (multiclass)\"\n",
        "\n",
        "    # Calculate average precision for PR curve\n",
        "    avg_precision = None\n",
        "    if y_pred_proba is not None:\n",
        "        try:\n",
        "            avg_precision = average_precision_score(y_test, y_pred_proba)\n",
        "        except Exception as e:\n",
        "            print(f\"  Warning: Could not calculate average precision: {e}\")\n",
        "            avg_precision = None\n",
        "\n",
        "    return acc, report_dict, cm, fpr, fnr, avg_precision\n",
        "\n",
        "# -----------------------------\n",
        "# Step 5: Save results to CSV\n",
        "# -----------------------------\n",
        "def save_results(name, acc, report_dict, cm, fpr, fnr, results_dir):\n",
        "    \"\"\"Save model results to CSV file\"\"\"\n",
        "    try:\n",
        "        csv_file = os.path.join(results_dir, f\"{name}_metrics.csv\")\n",
        "\n",
        "        with open(csv_file, mode='w', newline='', encoding='utf-8') as f:\n",
        "            writer = csv.writer(f)\n",
        "\n",
        "            writer.writerow([\"Metric\", \"Value\"])\n",
        "            writer.writerow([\"Overall Accuracy\", f\"{acc:.4f}\"])\n",
        "            writer.writerow([])\n",
        "\n",
        "            writer.writerow([\"Class-wise Metrics\"])\n",
        "            writer.writerow([\"Class\", \"Precision\", \"Recall\", \"F1-Score\", \"Support\"])\n",
        "\n",
        "            for class_label in sorted([k for k in report_dict.keys() if k.isdigit() or k == '0']):\n",
        "                if class_label in report_dict:\n",
        "                    metrics = report_dict[class_label]\n",
        "                    writer.writerow([\n",
        "                        f\"Class {class_label}\",\n",
        "                        f\"{metrics.get('precision', 0):.4f}\",\n",
        "                        f\"{metrics.get('recall', 0):.4f}\",\n",
        "                        f\"{metrics.get('f1-score', 0):.4f}\",\n",
        "                        metrics.get('support', 0)\n",
        "                    ])\n",
        "\n",
        "            writer.writerow([])\n",
        "            writer.writerow([\"Aggregate Metrics\"])\n",
        "            for avg_type in ['macro avg', 'weighted avg']:\n",
        "                if avg_type in report_dict:\n",
        "                    metrics = report_dict[avg_type]\n",
        "                    writer.writerow([\n",
        "                        avg_type,\n",
        "                        f\"{metrics.get('precision', 0):.4f}\",\n",
        "                        f\"{metrics.get('recall', 0):.4f}\",\n",
        "                        f\"{metrics.get('f1-score', 0):.4f}\",\n",
        "                        metrics.get('support', 0)\n",
        "                    ])\n",
        "\n",
        "            writer.writerow([])\n",
        "            writer.writerow([\"Error Rates\"])\n",
        "            writer.writerow([\"False Positive Rate (FPR)\", f\"{fpr:.4f}\" if isinstance(fpr, float) else fpr])\n",
        "            writer.writerow([\"False Negative Rate (FNR)\", f\"{fnr:.4f}\" if isinstance(fnr, float) else fnr])\n",
        "\n",
        "            writer.writerow([])\n",
        "            writer.writerow([\"Confusion Matrix\"])\n",
        "            writer.writerow([\"\"] + [f\"Predicted {i}\" for i in range(cm.shape[1])])\n",
        "            for i, row in enumerate(cm):\n",
        "                writer.writerow([f\"Actual {i}\"] + row.tolist())\n",
        "\n",
        "        print(f\"✓ {name} metrics saved to {csv_file}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving results for {name}: {e}\")\n",
        "\n",
        "# -----------------------------\n",
        "# Step 6: Generate Visualizations\n",
        "# -----------------------------\n",
        "def plot_confusion_matrix(cm, model_name, plots_dir):\n",
        "    \"\"\"Plot confusion matrix heatmap\"\"\"\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Safe (0)', 'Vulnerable (1)'],\n",
        "                yticklabels=['Safe (0)', 'Vulnerable (1)'],\n",
        "                cbar_kws={'label': 'Count'})\n",
        "    plt.title(f'Confusion Matrix - {model_name}', fontsize=14, fontweight='bold')\n",
        "    plt.ylabel('Actual Class', fontsize=12)\n",
        "    plt.xlabel('Predicted Class', fontsize=12)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    filename = os.path.join(plots_dir, f'{model_name}_confusion_matrix.png')\n",
        "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"  ✓ Confusion matrix saved: {filename}\")\n",
        "\n",
        "def generate_visualizations(all_results, plots_dir):\n",
        "    \"\"\"Generate all priority visualizations\"\"\"\n",
        "\n",
        "    model_names = list(all_results.keys())\n",
        "    metrics_data = {'Accuracy': [], 'Precision': [], 'Recall': [], 'F1-Score': []}\n",
        "    class_0_metrics = {'Precision': [], 'Recall': [], 'F1-Score': []}\n",
        "    class_1_metrics = {'Precision': [], 'Recall': [], 'F1-Score': []}\n",
        "    fpr_list = []\n",
        "    fnr_list = []\n",
        "\n",
        "    for model_name in model_names:\n",
        "        result = all_results[model_name]\n",
        "        report = result['report']\n",
        "\n",
        "        metrics_data['Accuracy'].append(result['accuracy'])\n",
        "        metrics_data['Precision'].append(report['weighted avg']['precision'])\n",
        "        metrics_data['Recall'].append(report['weighted avg']['recall'])\n",
        "        metrics_data['F1-Score'].append(report['weighted avg']['f1-score'])\n",
        "\n",
        "        class_0_metrics['Precision'].append(report['0']['precision'])\n",
        "        class_0_metrics['Recall'].append(report['0']['recall'])\n",
        "        class_0_metrics['F1-Score'].append(report['0']['f1-score'])\n",
        "\n",
        "        class_1_metrics['Precision'].append(report['1']['precision'])\n",
        "        class_1_metrics['Recall'].append(report['1']['recall'])\n",
        "        class_1_metrics['F1-Score'].append(report['1']['f1-score'])\n",
        "\n",
        "        fpr_list.append(result['fpr'])\n",
        "        fnr_list.append(result['fnr'])\n",
        "\n",
        "    # Multi-Metric Bar Chart\n",
        "    print(\"\\n  Generating multi-metric comparison...\")\n",
        "    fig, ax = plt.subplots(figsize=(12, 7))\n",
        "    x = np.arange(len(model_names))\n",
        "    width = 0.2\n",
        "\n",
        "    bars1 = ax.bar(x - 1.5*width, metrics_data['Accuracy'], width, label='Accuracy', color='#2ecc71')\n",
        "    bars2 = ax.bar(x - 0.5*width, metrics_data['Precision'], width, label='Precision', color='#3498db')\n",
        "    bars3 = ax.bar(x + 0.5*width, metrics_data['Recall'], width, label='Recall', color='#e74c3c')\n",
        "    bars4 = ax.bar(x + 1.5*width, metrics_data['F1-Score'], width, label='F1-Score', color='#f39c12')\n",
        "\n",
        "    ax.set_xlabel('Models', fontsize=12, fontweight='bold')\n",
        "    ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
        "    ax.set_title('Model Performance Comparison - All Metrics', fontsize=14, fontweight='bold')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(model_names, rotation=15, ha='right')\n",
        "    ax.legend(loc='lower right', fontsize=10)\n",
        "    ax.set_ylim([0, 1.05])\n",
        "    ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "    for bars in [bars1, bars2, bars3, bars4]:\n",
        "        for bar in bars:\n",
        "            height = bar.get_height()\n",
        "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                   f'{height:.3f}', ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    filename = os.path.join(plots_dir, 'multi_metric_comparison.png')\n",
        "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"  ✓ Multi-metric comparison saved: {filename}\")\n",
        "\n",
        "    # Precision-Recall Curves\n",
        "    print(\"  Generating precision-recall curves...\")\n",
        "    plt.figure(figsize=(10, 7))\n",
        "\n",
        "    has_curves = False\n",
        "    for model_name in model_names:\n",
        "        result = all_results[model_name]\n",
        "        if result['pr_curve'] is not None:\n",
        "            precision_vals, recall_vals, _ = result['pr_curve']\n",
        "            avg_precision = result['avg_precision']\n",
        "            if avg_precision is not None:\n",
        "                plt.plot(recall_vals, precision_vals, linewidth=2,\n",
        "                        label=f\"{model_name} (AP={avg_precision:.3f})\")\n",
        "            else:\n",
        "                plt.plot(recall_vals, precision_vals, linewidth=2, label=f\"{model_name}\")\n",
        "            has_curves = True\n",
        "\n",
        "    if has_curves:\n",
        "        plt.xlabel('Recall', fontsize=12, fontweight='bold')\n",
        "        plt.ylabel('Precision', fontsize=12, fontweight='bold')\n",
        "        plt.title('Precision-Recall Curves - All Models', fontsize=14, fontweight='bold')\n",
        "        plt.legend(loc='best', fontsize=10)\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.xlim([0, 1])\n",
        "        plt.ylim([0, 1.05])\n",
        "        plt.tight_layout()\n",
        "\n",
        "        filename = os.path.join(plots_dir, 'precision_recall_curves.png')\n",
        "        plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        print(f\"  ✓ Precision-recall curves saved: {filename}\")\n",
        "    else:\n",
        "        plt.close()\n",
        "        print(\"  ⚠ Skipping PR curves (no probability predictions available)\")\n",
        "\n",
        "    # FPR vs FNR Comparison\n",
        "    print(\"  Generating FPR vs FNR comparison...\")\n",
        "    fig, ax = plt.subplots(figsize=(12, 7))\n",
        "    x = np.arange(len(model_names))\n",
        "    width = 0.35\n",
        "\n",
        "    bars1 = ax.bar(x - width/2, fpr_list, width, label='False Positive Rate (FPR)',\n",
        "                   color='#e74c3c', alpha=0.8)\n",
        "    bars2 = ax.bar(x + width/2, fnr_list, width, label='False Negative Rate (FNR)',\n",
        "                   color='#c0392b', alpha=0.8)\n",
        "\n",
        "    ax.set_xlabel('Models', fontsize=12, fontweight='bold')\n",
        "    ax.set_ylabel('Error Rate', fontsize=12, fontweight='bold')\n",
        "    ax.set_title('False Positive Rate vs False Negative Rate', fontsize=14, fontweight='bold')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(model_names, rotation=15, ha='right')\n",
        "    ax.legend(fontsize=10)\n",
        "    ax.set_ylim([0, max(max(fpr_list), max(fnr_list)) * 1.2])\n",
        "    ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "    for bars in [bars1, bars2]:\n",
        "        for bar in bars:\n",
        "            height = bar.get_height()\n",
        "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                   f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "    ax.text(0.02, 0.98, '⚠️ Lower is better for both metrics\\nFNR is critical for security!',\n",
        "            transform=ax.transAxes, fontsize=10, verticalalignment='top',\n",
        "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    filename = os.path.join(plots_dir, 'fpr_vs_fnr_comparison.png')\n",
        "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"  ✓ FPR vs FNR comparison saved: {filename}\")\n",
        "\n",
        "    # Class-wise Performance Comparison\n",
        "    print(\"  Generating class-wise performance comparison...\")\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "    x = np.arange(len(model_names))\n",
        "    width = 0.25\n",
        "\n",
        "    # Class 0 (Safe)\n",
        "    ax = axes[0]\n",
        "    bars1 = ax.bar(x - width, class_0_metrics['Precision'], width, label='Precision', color='#3498db')\n",
        "    bars2 = ax.bar(x, class_0_metrics['Recall'], width, label='Recall', color='#2ecc71')\n",
        "    bars3 = ax.bar(x + width, class_0_metrics['F1-Score'], width, label='F1-Score', color='#9b59b6')\n",
        "\n",
        "    ax.set_xlabel('Models', fontsize=11, fontweight='bold')\n",
        "    ax.set_ylabel('Score', fontsize=11, fontweight='bold')\n",
        "    ax.set_title('Class 0 (Safe Code) Performance', fontsize=12, fontweight='bold')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(model_names, rotation=15, ha='right')\n",
        "    ax.legend(fontsize=9)\n",
        "    ax.set_ylim([0, 1.05])\n",
        "    ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # Class 1 (Vulnerable)\n",
        "    ax = axes[1]\n",
        "    bars1 = ax.bar(x - width, class_1_metrics['Precision'], width, label='Precision', color='#3498db')\n",
        "    bars2 = ax.bar(x, class_1_metrics['Recall'], width, label='Recall', color='#e74c3c')\n",
        "    bars3 = ax.bar(x + width, class_1_metrics['F1-Score'], width, label='F1-Score', color='#9b59b6')\n",
        "\n",
        "    ax.set_xlabel('Models', fontsize=11, fontweight='bold')\n",
        "    ax.set_ylabel('Score', fontsize=11, fontweight='bold')\n",
        "    ax.set_title('Class 1 (Vulnerable Code) Performance ⚠️', fontsize=12, fontweight='bold')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(model_names, rotation=15, ha='right')\n",
        "    ax.legend(fontsize=9)\n",
        "    ax.set_ylim([0, 1.05])\n",
        "    ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    filename = os.path.join(plots_dir, 'classwise_performance.png')\n",
        "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(f\"  ✓ Class-wise performance saved: {filename}\")\n",
        "\n",
        "    print(f\"\\n✅ All visualizations saved to '{plots_dir}/' directory\")\n",
        "\n",
        "# -----------------------------\n",
        "# Main execution\n",
        "# -----------------------------\n",
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "    # Load data\n",
        "    X_text, y = load_data(TRAIN_SIZE)\n",
        "\n",
        "    # Vectorize\n",
        "    X, vectorizer = vectorize_data(X_text, MAX_FEATURES)\n",
        "\n",
        "    # Split data\n",
        "    print(\"Splitting data...\")\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
        "    )\n",
        "\n",
        "    # Create results directory\n",
        "    os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "    os.makedirs(PLOTS_DIR, exist_ok=True)\n",
        "\n",
        "    # Get models\n",
        "    models = get_models()\n",
        "\n",
        "    # Store all results for visualization\n",
        "    all_results = {}\n",
        "\n",
        "    # Train and evaluate\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Training and Evaluating Models\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    for name, model in models.items():\n",
        "        try:\n",
        "            print(f\"\\nTraining {name}...\")\n",
        "            model.fit(X_train, y_train)\n",
        "\n",
        "            print(f\"Predicting with {name}...\")\n",
        "            y_pred = model.predict(X_test)\n",
        "\n",
        "            # Get probability predictions for PR curve\n",
        "            y_pred_proba = None\n",
        "            try:\n",
        "                if hasattr(model, \"predict_proba\"):\n",
        "                    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "                elif hasattr(model, \"decision_function\"):\n",
        "                    y_pred_proba = model.decision_function(X_test)\n",
        "            except Exception as e:\n",
        "                print(f\"  Warning: Could not get probability predictions: {e}\")\n",
        "\n",
        "            # Calculate metrics\n",
        "            acc, report_dict, cm, fpr, fnr, avg_precision = calculate_metrics(y_test, y_pred, y_pred_proba)\n",
        "\n",
        "            # Calculate PR curve data\n",
        "            pr_curve = None\n",
        "            if y_pred_proba is not None:\n",
        "                try:\n",
        "                    precision_vals, recall_vals, thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
        "                    pr_curve = (precision_vals, recall_vals, thresholds)\n",
        "                except Exception as e:\n",
        "                    print(f\"  Warning: Could not calculate PR curve: {e}\")\n",
        "\n",
        "            # Store results\n",
        "            all_results[name] = {\n",
        "                'accuracy': acc,\n",
        "                'report': report_dict,\n",
        "                'confusion_matrix': cm,\n",
        "                'fpr': fpr,\n",
        "                'fnr': fnr,\n",
        "                'pr_curve': pr_curve,\n",
        "                'avg_precision': avg_precision\n",
        "            }\n",
        "\n",
        "            # Save CSV results\n",
        "            save_results(name, acc, report_dict, cm, fpr, fnr, RESULTS_DIR)\n",
        "\n",
        "            # Generate confusion matrix plot\n",
        "            plot_confusion_matrix(cm, name, PLOTS_DIR)\n",
        "\n",
        "            print(f\"  Accuracy: {acc:.4f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"✗ Error with {name}: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            continue\n",
        "\n",
        "    # Generate comparison visualizations\n",
        "    if all_results:\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"Generating Comparison Visualizations\")\n",
        "        print(\"=\"*50)\n",
        "        generate_visualizations(all_results, PLOTS_DIR)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"✅ All results saved to '{RESULTS_DIR}/' directory\")\n",
        "    print(f\"✅ All plots saved to '{PLOTS_DIR}/' directory\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRnygxBTXB12",
        "outputId": "2b55be23-52e7-4c92-99da-cf8fcc73a954"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Devign dataset...\n",
            "Using FULL dataset: 21854 samples\n",
            "Loaded 21854 samples with 2 classes\n",
            "Vectorizing code...\n",
            "Feature matrix shape: (21854, 5000)\n",
            "Splitting data...\n",
            "\n",
            "==================================================\n",
            "Training and Evaluating Models\n",
            "==================================================\n",
            "\n",
            "Training Naive_Bayes...\n",
            "Predicting with Naive_Bayes...\n",
            "✓ Naive_Bayes metrics saved to ml_results_full_metrics/Naive_Bayes_metrics.csv\n",
            "  ✓ Confusion matrix saved: ml_plots/Naive_Bayes_confusion_matrix.png\n",
            "  Accuracy: 0.5884\n",
            "\n",
            "Training Logistic_Regression...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting with Logistic_Regression...\n",
            "✓ Logistic_Regression metrics saved to ml_results_full_metrics/Logistic_Regression_metrics.csv\n",
            "  ✓ Confusion matrix saved: ml_plots/Logistic_Regression_confusion_matrix.png\n",
            "  Accuracy: 0.5795\n",
            "\n",
            "Training Random_Forest...\n",
            "Predicting with Random_Forest...\n",
            "✓ Random_Forest metrics saved to ml_results_full_metrics/Random_Forest_metrics.csv\n",
            "  ✓ Confusion matrix saved: ml_plots/Random_Forest_confusion_matrix.png\n",
            "  Accuracy: 0.5841\n",
            "\n",
            "Training XGBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [13:15:24] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting with XGBoost...\n",
            "✓ XGBoost metrics saved to ml_results_full_metrics/XGBoost_metrics.csv\n",
            "  ✓ Confusion matrix saved: ml_plots/XGBoost_confusion_matrix.png\n",
            "  Accuracy: 0.5896\n",
            "\n",
            "==================================================\n",
            "Generating Comparison Visualizations\n",
            "==================================================\n",
            "\n",
            "  Generating multi-metric comparison...\n",
            "  ✓ Multi-metric comparison saved: ml_plots/multi_metric_comparison.png\n",
            "  Generating precision-recall curves...\n",
            "  ✓ Precision-recall curves saved: ml_plots/precision_recall_curves.png\n",
            "  Generating FPR vs FNR comparison...\n",
            "  ✓ FPR vs FNR comparison saved: ml_plots/fpr_vs_fnr_comparison.png\n",
            "  Generating class-wise performance comparison...\n",
            "  ✓ Class-wise performance saved: ml_plots/classwise_performance.png\n",
            "\n",
            "✅ All visualizations saved to 'ml_plots/' directory\n",
            "\n",
            "==================================================\n",
            "✅ All results saved to 'ml_results_full_metrics/' directory\n",
            "✅ All plots saved to 'ml_plots/' directory\n",
            "==================================================\n"
          ]
        }
      ]
    }
  ]
}