{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNAWXXP0vqgZ0LCXpUSkl5c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShamaSharma/SVD/blob/main/fusevulwithembedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bshey_LFPM3Q",
        "outputId": "c9319b89-6b8f-4121-e9e5-9c27fe1fe86f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.19.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.35.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "CUDA available: True\n",
            "GPU: Tesla T4\n",
            "GPU Memory: 14.7 GB\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install transformers\n",
        "!pip install pandas scikit-learn tqdm\n",
        "!pip install datasets\n",
        "\n",
        "# Check GPU availability\n",
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "else:\n",
        "    print(\"Using CPU\")\n",
        "\n",
        "# Set up runtime type - you should manually set GPU in Colab settings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "print(\"Upload your CSV files or ZIP containing the dataset folder:\")\n",
        "print(\"Expected files:\")\n",
        "print(\"- devign_train_normalized.csv\")\n",
        "print(\"- devign_val_normalized.csv\")\n",
        "print(\"- ss_train.csv\")\n",
        "print(\"- ss_val.csv\")\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "# If you upload a ZIP file, extract it\n",
        "for filename in uploaded.keys():\n",
        "    if filename.endswith('.zip'):\n",
        "        print(f\"Extracting {filename}...\")\n",
        "        with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "            zip_ref.extractall('.')\n",
        "        print(f\"Extracted {filename}\")\n",
        "\n",
        "# Check current directory structure\n",
        "print(\"\\nCurrent directory structure:\")\n",
        "for root, dirs, files in os.walk('.'):\n",
        "    level = root.replace('.', '').count(os.sep)\n",
        "    indent = ' ' * 2 * level\n",
        "    print(f\"{indent}{os.path.basename(root)}/\")\n",
        "    subindent = ' ' * 2 * (level + 1)\n",
        "    for file in files:\n",
        "        if file.endswith('.csv'):\n",
        "            print(f\"{subindent}{file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "YI-DONeXPsn5",
        "outputId": "1dc03c01-15b3-4095-82c1-3ed734fa4929"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload your CSV files or ZIP containing the dataset folder:\n",
            "Expected files:\n",
            "- devign_train_normalized.csv\n",
            "- devign_val_normalized.csv\n",
            "- ss_train.csv\n",
            "- ss_val.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c2cbb7bd-dac8-49d2-9009-be484350e328\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c2cbb7bd-dac8-49d2-9009-be484350e328\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving devign_train_normalized.csv to devign_train_normalized.csv\n",
            "Saving devign_val_normalized.csv to devign_val_normalized.csv\n",
            "Saving ss_train.csv to ss_train.csv\n",
            "Saving ss_val.csv to ss_val.csv\n",
            "\n",
            "Current directory structure:\n",
            "./\n",
            "  devign_val_normalized.csv\n",
            "  ss_val.csv\n",
            "  ss_train.csv\n",
            "  devign_train_normalized.csv\n",
            "  .config/\n",
            "    configurations/\n",
            "    logs/\n",
            "      2025.09.25/\n",
            "  sample_data/\n",
            "    california_housing_train.csv\n",
            "    mnist_test.csv\n",
            "    california_housing_test.csv\n",
            "    mnist_train_small.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile selfattention.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttention(torch.nn.Module):\n",
        "    def __init__(self, embed_size, dimen_size):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.dimen_size = dimen_size\n",
        "\n",
        "        # Initialize weight matrices\n",
        "        self.values = torch.nn.Linear(embed_size, embed_size, bias=False)\n",
        "        self.keys = torch.nn.Linear(embed_size, embed_size, bias=False)\n",
        "        self.queries = torch.nn.Linear(embed_size, embed_size, bias=False)\n",
        "        self.assist = torch.nn.Linear(embed_size, dimen_size, bias=False)\n",
        "\n",
        "    def forward(self, code_output, text_output):\n",
        "        outs = []\n",
        "        for i in range(code_output.shape[0]):\n",
        "            # Extract feature vectors for each code\n",
        "            code = code_output[i].unsqueeze(0)\n",
        "            text = text_output[i].unsqueeze(0)\n",
        "\n",
        "            values = self.values(code)\n",
        "            keys = self.keys(code)\n",
        "            queries = self.queries(code)\n",
        "            assist = self.assist(text)\n",
        "\n",
        "            # Calculate attention scores\n",
        "            attention = torch.matmul(queries, keys.permute(0, 2, 1))\n",
        "            attention = torch.matmul(attention, assist)\n",
        "            attention = attention / (self.embed_size ** 0.5)\n",
        "\n",
        "            # Use softmax function to calculate attention weights\n",
        "            attention = nn.functional.softmax(attention, dim=-1)\n",
        "            # Use weights for weighted average of values\n",
        "            out = torch.matmul(attention, values)\n",
        "\n",
        "            outs.append(out)\n",
        "        output = torch.cat(outs, dim=0)\n",
        "        return output\n",
        "\n",
        "print(\"✅ selfattention.py created successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VyFLYhAP95u",
        "outputId": "486bc3bf-c308-49bc-b2eb-601edddcc37a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing selfattention.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile model.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from selfattention import SelfAttention\n",
        "\n",
        "class Code_Note(nn.Module):\n",
        "    def __init__(self, code_encoder, text_encoder, input_size, hidden_size, output_size):\n",
        "        super(Code_Note, self).__init__()\n",
        "        self.code_encoder = code_encoder\n",
        "        self.text_encoder = text_encoder\n",
        "        self.attention = SelfAttention(768, 512)\n",
        "        self.fc1 = torch.nn.Linear(input_size, hidden_size)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.fc2 = torch.nn.Linear(hidden_size, output_size)\n",
        "        self.relu2 = torch.nn.ReLU()\n",
        "        self.fc3 = torch.nn.Linear(output_size, 2)\n",
        "\n",
        "        # Initialize embedding storage\n",
        "        self.last_code_embeddings = None\n",
        "        self.last_text_embeddings = None\n",
        "        self.last_attention_embeddings = None\n",
        "        self.last_pooled_embeddings = None\n",
        "\n",
        "    def forward(self, inputs_code_id, inputs_code_mask, inputs_text_id, inputs_text_mask, save_embeddings=False):\n",
        "        code_output = self.code_encoder.encoder(inputs_code_id, attention_mask=inputs_code_mask).last_hidden_state\n",
        "        text_output = self.text_encoder(inputs_text_id, attention_mask=inputs_text_mask).last_hidden_state\n",
        "\n",
        "        # Apply attention mechanism\n",
        "        attention_output = self.attention(code_output, text_output)\n",
        "        pooled_output = attention_output[:, 0, :]  # Take CLS token\n",
        "\n",
        "        # Save embeddings if requested\n",
        "        if save_embeddings:\n",
        "            self.last_code_embeddings = code_output.detach().cpu()\n",
        "            self.last_text_embeddings = text_output.detach().cpu()\n",
        "            self.last_attention_embeddings = attention_output.detach().cpu()\n",
        "            self.last_pooled_embeddings = pooled_output.detach().cpu()\n",
        "\n",
        "        # Pass through fully connected layers\n",
        "        output = self.fc1(pooled_output)\n",
        "        output = self.relu(output)\n",
        "        output = self.fc2(output)\n",
        "        output = self.relu2(output)\n",
        "        output = self.fc3(output)\n",
        "        return output\n",
        "\n",
        "print(\"✅ model.py created successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Jol9kMwRRBG",
        "outputId": "2ef4926c-f9a4-4c38-d423-fe541d3583a9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile run.py\n",
        "import logging\n",
        "import pandas as pd\n",
        "import torch\n",
        "import time\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "from model import Code_Note\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import AutoTokenizer, AutoModel, RobertaTokenizer, RobertaModel\n",
        "import warnings\n",
        "import sklearn.exceptions\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n",
        "\n",
        "class Example(object):\n",
        "    def __init__(self, code, text, label, idx=None):\n",
        "        self.code = code\n",
        "        self.text = text\n",
        "        self.label = label\n",
        "        self.idx = idx\n",
        "\n",
        "class InputFeatures(object):\n",
        "    def __init__(self, inputs_code_ids, inputs_code_masks, inputs_text_ids, inputs_text_masks, label, idx=None):\n",
        "        self.inputs_code_ids = inputs_code_ids\n",
        "        self.inputs_code_masks = inputs_code_masks\n",
        "        self.inputs_text_ids = inputs_text_ids\n",
        "        self.inputs_text_masks = inputs_text_masks\n",
        "        self.label = label\n",
        "        self.idx = idx\n",
        "\n",
        "def read_file(codefile, textfile):\n",
        "    examples = []\n",
        "    print(f\"Reading code file: {codefile}\")\n",
        "    print(f\"Reading text file: {textfile}\")\n",
        "\n",
        "    code_data = pd.read_csv(codefile, na_filter=False, encoding_errors='ignore')\n",
        "    text_data = pd.read_csv(textfile, na_filter=False, encoding_errors='ignore')\n",
        "\n",
        "    print(f\"Code data shape: {code_data.shape}\")\n",
        "    print(f\"Text data shape: {text_data.shape}\")\n",
        "\n",
        "    code = code_data['code'].values.tolist()\n",
        "    code_label = code_data['label'].values.tolist()\n",
        "    text = text_data['text'].values.tolist()\n",
        "    text_label = text_data['label'].values.tolist()\n",
        "\n",
        "    for idx, (c, cl, t, tl) in enumerate(zip(code, code_label, text, text_label)):\n",
        "        if c != '' and t != '' and int(cl) == int(tl):\n",
        "            examples.append(Example(c, t, int(cl), idx))\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    print(f\"Created {len(examples)} examples\")\n",
        "    return examples\n",
        "\n",
        "def mini_sample(examples, num):\n",
        "    if num >= len(examples):\n",
        "        print(f\"Sample size ({num}) >= total examples ({len(examples)}), using all examples\")\n",
        "        return examples\n",
        "    example1 = []\n",
        "    unique_numbers = random.sample(range(0, len(examples)), num)\n",
        "    for n in unique_numbers:\n",
        "        for example_index, example in enumerate(examples):\n",
        "            if example_index == n:\n",
        "                example1.append(example)\n",
        "    print(f\"Sampled {len(example1)} examples from {len(examples)} total\")\n",
        "    return example1\n",
        "\n",
        "def text_to_feature(examples, code_tokenizer, text_tokenizer, stage=None):\n",
        "    features = []\n",
        "    print(f\"Converting {len(examples)} examples to features...\")\n",
        "    for example_index, example in enumerate(examples):\n",
        "        code_tokens = code_tokenizer.tokenize(example.code)[:510]\n",
        "        code_tokens = [code_tokenizer.cls_token] + code_tokens + [code_tokenizer.sep_token]\n",
        "        inputs_code_ids = code_tokenizer.convert_tokens_to_ids(code_tokens)\n",
        "        inputs_code_masks = [1] * len(code_tokens)\n",
        "        code_padding_length = 512 - len(inputs_code_ids)\n",
        "        inputs_code_ids += [code_tokenizer.pad_token_id] * code_padding_length\n",
        "        inputs_code_masks += [0] * code_padding_length\n",
        "\n",
        "        text_tokens = text_tokenizer.tokenize(example.text)[:510]\n",
        "        text_tokens = [text_tokenizer.cls_token] + text_tokens + [text_tokenizer.sep_token]\n",
        "        inputs_text_ids = text_tokenizer.convert_tokens_to_ids(text_tokens)\n",
        "        inputs_text_masks = [1] * len(text_tokens)\n",
        "        text_padding_length = 512 - len(inputs_text_ids)\n",
        "        inputs_text_ids += [text_tokenizer.pad_token_id] * text_padding_length\n",
        "        inputs_text_masks += [0] * text_padding_length\n",
        "\n",
        "        features.append(InputFeatures(\n",
        "            inputs_code_ids,\n",
        "            inputs_code_masks,\n",
        "            inputs_text_ids,\n",
        "            inputs_text_masks,\n",
        "            example.label,\n",
        "            example.idx\n",
        "        ))\n",
        "    print(f\"Created {len(features)} features\")\n",
        "    return features\n",
        "\n",
        "def extract_and_save_embeddings(model, dataloader, device, save_dir, split_name):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    model.eval()\n",
        "    all_code_embeddings, all_text_embeddings, all_attention_embeddings, all_pooled_embeddings = [], [], [], []\n",
        "    all_labels, all_indices = [], []\n",
        "    print(f\"\\nExtracting embeddings for {split_name}...\")\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(tqdm(dataloader, desc=f'Extracting {split_name} embeddings')):\n",
        "            inputs_code_id, inputs_code_mask, inputs_text_id, inputs_text_mask, inputs_label, indices = [x.to(device) for x in batch]\n",
        "            all_indices.extend(indices.cpu().numpy())\n",
        "            _ = model(inputs_code_id, inputs_code_mask, inputs_text_id, inputs_text_mask, save_embeddings=True)\n",
        "            all_code_embeddings.append(model.last_code_embeddings)\n",
        "            all_text_embeddings.append(model.last_text_embeddings)\n",
        "            all_attention_embeddings.append(model.last_attention_embeddings)\n",
        "            all_pooled_embeddings.append(model.last_pooled_embeddings)\n",
        "            all_labels.extend(inputs_label.cpu().numpy())\n",
        "    all_code_embeddings = torch.cat(all_code_embeddings, dim=0)\n",
        "    all_text_embeddings = torch.cat(all_text_embeddings, dim=0)\n",
        "    all_attention_embeddings = torch.cat(all_attention_embeddings, dim=0)\n",
        "    all_pooled_embeddings = torch.cat(all_pooled_embeddings, dim=0)\n",
        "    embeddings_dict = {\n",
        "        'code_embeddings_full': all_code_embeddings.numpy(),\n",
        "        'text_embeddings_full': all_text_embeddings.numpy(),\n",
        "        'attention_embeddings_full': all_attention_embeddings.numpy(),\n",
        "        'code_embeddings_cls': all_code_embeddings[:, 0, :].numpy(),\n",
        "        'text_embeddings_cls': all_text_embeddings[:, 0, :].numpy(),\n",
        "        'attention_embeddings_cls': all_attention_embeddings[:, 0, :].numpy(),\n",
        "        'code_embeddings_mean': all_code_embeddings.mean(dim=1).numpy(),\n",
        "        'text_embeddings_mean': all_text_embeddings.mean(dim=1).numpy(),\n",
        "        'attention_embeddings_mean': all_attention_embeddings.mean(dim=1).numpy(),\n",
        "        'pooled_embeddings': all_pooled_embeddings.numpy(),\n",
        "        'code_text_cls_concat': np.concatenate([\n",
        "            all_code_embeddings[:, 0, :].numpy(),\n",
        "            all_text_embeddings[:, 0, :].numpy()\n",
        "        ], axis=1),\n",
        "        'labels': np.array(all_labels),\n",
        "        'indices': np.array(all_indices),\n",
        "    }\n",
        "    filename = f'{split_name}_embeddings_final.npz'\n",
        "    save_path = os.path.join(save_dir, filename)\n",
        "    np.savez_compressed(save_path, **embeddings_dict)\n",
        "    print(f\"Saved {split_name} embeddings to {save_path}\")\n",
        "    return embeddings_dict\n",
        "\n",
        "def evaluate(eval_dataloader, model, device):\n",
        "    start_time = time.time()\n",
        "    total_correct, total_examples = 0.0, 0.0\n",
        "    all_pre, all_labels = [], []\n",
        "    model.eval()\n",
        "    for batch in eval_dataloader:\n",
        "        inputs_code_id, inputs_code_mask, inputs_text_id, inputs_text_mask, inputs_label = [x.to(device) for x in batch]\n",
        "        mlp_output = model(inputs_code_id, inputs_code_mask, inputs_text_id, inputs_text_mask, save_embeddings=False)\n",
        "        pred = torch.argmax(mlp_output, dim=1)\n",
        "        all_labels += inputs_label.tolist()\n",
        "        all_pre += pred.tolist()\n",
        "        correct = torch.sum(pred == inputs_label)\n",
        "        total_correct += correct.item()\n",
        "        total_examples += int(mlp_output.size(0))\n",
        "    acc = total_correct / total_examples\n",
        "    f1 = f1_score(y_true=all_labels, y_pred=all_pre)\n",
        "    rec = recall_score(y_true=all_labels, y_pred=all_pre)\n",
        "    prec = precision_score(y_true=all_labels, y_pred=all_pre)\n",
        "    return {'acc': acc, 'f1': f1, 'rec': rec, 'prec': prec, 'execution_time': time.time() - start_time}\n",
        "\n",
        "def main():\n",
        "    print(\"Starting training with optimized embedding extraction...\")\n",
        "    epochs, batchsize = 10, 4\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "    print(\"Loading pre-trained models...\")\n",
        "    code_tokenizer = AutoTokenizer.from_pretrained('Salesforce/codet5p-110m-embedding', trust_remote_code=True)\n",
        "    code_model = AutoModel.from_pretrained('Salesforce/codet5p-110m-embedding', trust_remote_code=True)\n",
        "    text_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "    text_model = RobertaModel.from_pretrained('roberta-base')\n",
        "    model = Code_Note(code_model, text_model, 768, 1536, 384)\n",
        "    model.to(device)\n",
        "    print(\"Models loaded successfully!\")\n",
        "\n",
        "    train_codefile, train_textfile = 'devign_train_normalized.csv', 'ss_train.csv'\n",
        "    eval_codefile, eval_textfile = 'devign_val_normalized.csv', 'ss_val.csv'\n",
        "    print(\"Checking if files exist...\")\n",
        "    for file in [train_codefile, train_textfile, eval_codefile, eval_textfile]:\n",
        "        if os.path.exists(file):\n",
        "            print(f\"✓ Found: {file}\")\n",
        "        else:\n",
        "            print(f\"✗ Missing: {file}\")\n",
        "            return\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=2e-6)\n",
        "\n",
        "    print(\"\\nLoading training data...\")\n",
        "    examples = read_file(train_codefile, train_textfile)\n",
        "    examples = mini_sample(examples, 2000)\n",
        "    print(f\"Using {len(examples)} training samples\")\n",
        "    train_examples = text_to_feature(examples, code_tokenizer, text_tokenizer, 'train')\n",
        "    all_inputs_code_ids = torch.tensor([f.inputs_code_ids for f in train_examples])\n",
        "    all_inputs_code_masks = torch.tensor([f.inputs_code_masks for f in train_examples])\n",
        "    all_inputs_text_ids = torch.tensor([f.inputs_text_ids for f in train_examples])\n",
        "    all_inputs_text_masks = torch.tensor([f.inputs_text_masks for f in train_examples])\n",
        "    all_inputs_labels = torch.tensor([f.label for f in train_examples])\n",
        "    all_indices = torch.tensor([f.idx for f in train_examples])\n",
        "    train_data = TensorDataset(all_inputs_code_ids, all_inputs_code_masks, all_inputs_text_ids, all_inputs_text_masks, all_inputs_labels)\n",
        "    train_dataloader = DataLoader(train_data, batch_size=batchsize, shuffle=True)\n",
        "\n",
        "    print(\"\\nLoading validation data...\")\n",
        "    eva_examples = read_file(eval_codefile, eval_textfile)\n",
        "    print(f\"Using {len(eva_examples)} validation samples\")\n",
        "    eval_examples = text_to_feature(eva_examples, code_tokenizer, text_tokenizer, 'eval')\n",
        "    all_evalinputs_code_ids = torch.tensor([f.inputs_code_ids for f in eval_examples])\n",
        "    all_evalinputs_code_masks = torch.tensor([f.inputs_code_masks for f in eval_examples])\n",
        "    all_evalinputs_text_ids = torch.tensor([f.inputs_text_ids for f in eval_examples])\n",
        "    all_evalinputs_text_masks = torch.tensor([f.inputs_text_masks for f in eval_examples])\n",
        "    all_evalinputs_labels = torch.tensor([f.label for f in eval_examples])\n",
        "    all_eval_indices = torch.tensor([f.idx for f in eval_examples])\n",
        "    eval_data = TensorDataset(all_evalinputs_code_ids, all_evalinputs_code_masks, all_evalinputs_text_ids, all_evalinputs_text_masks, all_evalinputs_labels)\n",
        "    eval_dataloader = DataLoader(eval_data, batch_size=batchsize, shuffle=False)\n",
        "\n",
        "    os.makedirs('embeddings', exist_ok=True)\n",
        "    best_metrics, best_epoch = {}, 0\n",
        "    print(f\"\\nStarting training for {epochs} epochs...\")\n",
        "    for epoch in range(epochs):\n",
        "        train_total_lose, train_total_correct, train_total_examples = 0.0, 0.0, 0.0\n",
        "        model.train()\n",
        "        loop = tqdm(train_dataloader, total=len(train_dataloader))\n",
        "        for bidx, batch in enumerate(loop):\n",
        "            inputs_code_id, inputs_code_mask, inputs_text_id, inputs_text_mask, inputs_label = [x.to(device) for x in batch]\n",
        "            optimizer.zero_grad()\n",
        "            mlp_output = model(inputs_code_id, inputs_code_mask, inputs_text_id, inputs_text_mask, save_embeddings=False)\n",
        "            loss = criterion(mlp_output, inputs_label)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            pred = torch.argmax(mlp_output, dim=1)\n",
        "            train_total_lose += loss.item()\n",
        "            correct = torch.sum(pred == inputs_label)\n",
        "            train_total_correct += correct.item()\n",
        "            train_total_examples += int(mlp_output.size(0))\n",
        "            loop.set_description(f'Epoch [{epoch+1}/{epochs}]')\n",
        "            loop.set_postfix({'Train Loss': f'{train_total_lose/(bidx+1):.4f}', 'Train ACC': f'{train_total_correct/train_total_examples:.4f}'})\n",
        "        metrics = evaluate(eval_dataloader, model, device)\n",
        "        eval_acc, eval_f1, eval_rec, eval_prec, eval_time = metrics['acc'], metrics['f1'], metrics['rec'], metrics['prec'], metrics['execution_time']\n",
        "        print(f'Epoch [{epoch + 1}/{epochs}] val_time: {eval_time:.2f}s val_acc={eval_acc:.4f}, val_f1={eval_f1:.4f}, val_recall={eval_rec:.4f}, val_precision={eval_prec:.4f}')\n",
        "        if epoch == 0 or eval_acc >= best_metrics.get('acc', 0):\n",
        "            best_metrics, best_epoch = metrics, epoch\n",
        "            torch.save({'epoch': epoch,'model_state_dict': model.state_dict(),'optimizer_state_dict': optimizer.state_dict(),'best_metrics': best_metrics}, 'best_model.pth')\n",
        "        print(f'Best epoch: {best_epoch+1} acc={best_metrics[\"acc\"]:.4f}, f1={best_metrics[\"f1\"]:.4f}\\n')\n",
        "\n",
        "    print(\"\\nTRAINING COMPLETED! Now extracting embeddings...\")\n",
        "    train_data_indices = TensorDataset(all_inputs_code_ids, all_inputs_code_masks, all_inputs_text_ids, all_inputs_text_masks, all_inputs_labels, all_indices)\n",
        "    train_dataloader_indices = DataLoader(train_data_indices, batch_size=batchsize, shuffle=False)\n",
        "    eval_data_indices = TensorDataset(all_evalinputs_code_ids, all_evalinputs_code_masks, all_evalinputs_text_ids, all_evalinputs_text_masks, all_evalinputs_labels, all_eval_indices)\n",
        "    eval_dataloader_indices = DataLoader(eval_data_indices, batch_size=batchsize, shuffle=False)\n",
        "    train_embeddings = extract_and_save_embeddings(model, train_dataloader_indices, device, 'embeddings', 'train')\n",
        "    val_embeddings = extract_and_save_embeddings(model, eval_dataloader_indices, device, 'embeddings', 'val')\n",
        "    print(\"\\nAll done! Best model saved: 'best_model.pth', embeddings saved in 'embeddings/'\")\n",
        "    return train_embeddings, val_embeddings\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_emb, val_emb = main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dempELbbVy-A",
        "outputId": "a9f2eb49-9518-4850-dda3-9ad3fe122665"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing run.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute the training script\n",
        "exec(open('run.py').read())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19t0cNksTYb9",
        "outputId": "757571b9-fdef-412c-b5db-be11070c33f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training with optimized embedding extraction...\n",
            "Using device: cuda\n",
            "Loading pre-trained models...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Models loaded successfully!\n",
            "Checking if files exist...\n",
            "✓ Found: devign_train_normalized.csv\n",
            "✓ Found: ss_train.csv\n",
            "✓ Found: devign_val_normalized.csv\n",
            "✓ Found: ss_val.csv\n",
            "\n",
            "Loading training data...\n",
            "Reading code file: devign_train_normalized.csv\n",
            "Reading text file: ss_train.csv\n",
            "Code data shape: (21854, 2)\n",
            "Text data shape: (21854, 2)\n",
            "Created 21854 examples\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1294 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sampled 2000 examples from 21854 total\n",
            "Using 2000 training samples\n",
            "Converting 2000 examples to features...\n",
            "Created 2000 features\n",
            "\n",
            "Loading validation data...\n",
            "Reading code file: devign_val_normalized.csv\n",
            "Reading text file: ss_val.csv\n",
            "Code data shape: (2732, 2)\n",
            "Text data shape: (2732, 2)\n",
            "Created 2732 examples\n",
            "Using 2732 validation samples\n",
            "Converting 2732 examples to features...\n",
            "Created 2732 features\n",
            "\n",
            "Starting training for 10 epochs...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [1/10]: 100%|██████████| 500/500 [07:00<00:00,  1.19it/s, Train Loss=0.6915, Train ACC=0.5225]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10] val_time: 183.78s val_acc=0.5373, val_f1=0.3667, val_recall=0.3083, val_precision=0.4524\n",
            "Best epoch: 1 acc=0.5373, f1=0.3667\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [2/10]: 100%|██████████| 500/500 [07:06<00:00,  1.17it/s, Train Loss=0.6889, Train ACC=0.5380]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/10] val_time: 183.47s val_acc=0.5311, val_f1=0.3955, val_recall=0.3530, val_precision=0.4496\n",
            "Best epoch: 1 acc=0.5373, f1=0.3667\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [3/10]:  31%|███       | 156/500 [02:13<04:52,  1.18it/s, Train Loss=0.6899, Train ACC=0.5272]"
          ]
        }
      ]
    }
  ]
}